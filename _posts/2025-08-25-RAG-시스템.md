---
title: "RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ"
date: 2025-08-25 19:00:00 +0900
categories: [AI, NLP, RAG]
tags: [RAG, ê²€ìƒ‰ì¦ê°•ìƒì„±, LangChain, ë²¡í„°ê²€ìƒ‰, LLM]
---

# RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ

RAGëŠ” ìµœê·¼ AI ë¶„ì•¼ì—ì„œ ê°€ì¥ ì£¼ëª©ë°›ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” RAGì˜ ê°œë…ë¶€í„° ì‹¤ì œ êµ¬í˜„ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

## ğŸ¤” RAGë€ ë¬´ì—‡ì¸ê°€?

**RAG (Retrieval-Augmented Generation)**ëŠ” **ê²€ìƒ‰ ì¦ê°• ìƒì„±**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 

### ê¸°ì¡´ LLMì˜ í•œê³„
- **í›ˆë ¨ ë°ì´í„° ì‹œì  ê³ ì •**: íŠ¹ì • ì‹œì ê¹Œì§€ì˜ ë°ì´í„°ë§Œ í•™ìŠµ
- **ë„ë©”ì¸ ì§€ì‹ ë¶€ì¡±**: íŠ¹ì • ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ ë¶€ì¡±
- **í™˜ê°(Hallucination)**: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œ

### RAGì˜ í•´ê²°ì±…
```
ì‚¬ìš©ì ì§ˆë¬¸ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ â†’ LLMì´ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
```

## ğŸ—ï¸ RAG ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ

```mermaid
graph TD
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[ì§ˆë¬¸ ë²¡í„°í™”]
    B --> C[ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰]
    C --> D[ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰]
    D --> E[ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±]
    E --> F[LLMì— ì „ë‹¬]
    F --> G[ìµœì¢… ë‹µë³€ ìƒì„±]
```

### ìƒì„¸ í”„ë¡œì„¸ìŠ¤

1. **ë¬¸ì„œ ì²˜ë¦¬ (Document Processing)**
   - ì›ë³¸ ë¬¸ì„œë¥¼ ì²­í¬(chunk)ë¡œ ë¶„í• 
   - ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©
   - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥

2. **ì§ˆì˜ ì²˜ë¦¬ (Query Processing)**
   - ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
   - ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°

3. **ìƒì„± (Generation)**
   - ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
   - LLMì´ ìµœì¢… ë‹µë³€ ìƒì„±

## ğŸ” ê²€ìƒ‰ ë°©ì‹ ë¹„êµ

### 1. ë²¡í„° ê²€ìƒ‰ (Vector Search)

**ì¥ì :**
- ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ìƒ‰ ê°€ëŠ¥
- ë‹¤êµ­ì–´ ì§€ì›
- ë¬¸ë§¥ ì´í•´ ëŠ¥ë ¥

**ë‹¨ì :**
- ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì–´ë ¤ì›€
- ê³„ì‚° ë¹„ìš© ë†’ìŒ

```python
# ë²¡í„° ê²€ìƒ‰ ì˜ˆì‹œ
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

# ë¬¸ì„œ ì„ë² ë”©
documents = ["ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì…ë‹ˆë‹¤", "ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤"]
doc_embeddings = model.encode(documents)

# ì§ˆë¬¸ ì„ë² ë”©
query = "AI ê¸°ìˆ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
query_embedding = model.encode([query])

# ìœ ì‚¬ë„ ê³„ì‚°
similarities = np.dot(query_embedding, doc_embeddings.T)
```

### 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (BM25)

**ì¥ì :**
- ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
- ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„
- í•´ì„ ê°€ëŠ¥ì„±

**ë‹¨ì :**
- ë™ì˜ì–´ ì²˜ë¦¬ ì–´ë ¤ì›€
- ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¶€ì¡±

### 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

**ë²¡í„° ê²€ìƒ‰ + BM25**ë¥¼ ê²°í•©í•˜ì—¬ ë‘ ë°©ì‹ì˜ ì¥ì ì„ ëª¨ë‘ í™œìš©:

```python
# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜ˆì‹œ
def hybrid_search(query, documents, vector_weight=0.7, bm25_weight=0.3):
    # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜
    vector_scores = vector_search(query, documents)
    
    # BM25 ê²€ìƒ‰ ì ìˆ˜
    bm25_scores = bm25_search(query, documents)
    
    # ê°€ì¤‘ í‰ê· 
    final_scores = vector_weight * vector_scores + bm25_weight * bm25_scores
    
    return final_scores
```

## ğŸ› ï¸ ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ

### 1. LangChainì„ í™œìš©í•œ RAG ì‹œìŠ¤í…œ

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# 1. ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
loader = TextLoader("document.txt")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
texts = text_splitter.split_documents(documents)

# 2. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(texts, embeddings)

# 3. RAG ì²´ì¸ êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 4. ì§ˆì˜ì‘ë‹µ
result = qa_chain.run("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œìš”?")
print(result)
```

### 2. í•œêµ­ì–´ íŠ¹í™” RAG ì‹œìŠ¤í…œ

```python
from transformers import AutoTokenizer, AutoModel
import torch

# í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sroberta-multitask')
model = AutoModel.from_pretrained('jhgan/ko-sroberta-multitask')

def get_korean_embedding(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

# í•œêµ­ì–´ ë¬¸ì„œ ì„ë² ë”©
korean_docs = ["ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤", 
               "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤"]
korean_embeddings = [get_korean_embedding(doc) for doc in korean_docs]
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. ì²­í¬ í¬ê¸° ìµœì í™”

```python
# ì²­í¬ í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ
chunk_sizes = [200, 500, 1000, 2000]
results = []

for chunk_size in chunk_sizes:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_size // 10
    )
    chunks = splitter.split_documents(documents)
    
    # ì„±ëŠ¥ ì¸¡ì •
    accuracy = evaluate_rag_system(chunks)
    results.append((chunk_size, accuracy))

# ìµœì  ì²­í¬ í¬ê¸° ì„ íƒ
optimal_chunk_size = max(results, key=lambda x: x[1])[0]
```

### 2. ì¬ë­í‚¹ (Re-ranking)

```python
from sentence_transformers import CrossEncoder

# ì¬ë­í‚¹ ëª¨ë¸
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_documents(query, documents, top_k=5):
    # ì´ˆê¸° ê²€ìƒ‰
    initial_results = vector_search(query, documents, top_k=20)
    
    # ì¬ë­í‚¹
    pairs = [(query, doc) for doc in initial_results]
    scores = reranker.predict(pairs)
    
    # ìƒìœ„ kê°œ ì„ íƒ
    reranked_indices = np.argsort(scores)[::-1][:top_k]
    return [initial_results[i] for i in reranked_indices]
```

### 3. ë©”íƒ€ë°ì´í„° í•„í„°ë§

```python
# ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•œ í•„í„°ë§
def filtered_search(query, documents, metadata_filter):
    # ë²¡í„° ê²€ìƒ‰
    vector_results = vector_search(query, documents)
    
    # ë©”íƒ€ë°ì´í„° í•„í„°ë§
    filtered_results = []
    for doc, score in vector_results:
        if matches_metadata(doc.metadata, metadata_filter):
            filtered_results.append((doc, score))
    
    return filtered_results

# ì‚¬ìš© ì˜ˆì‹œ
metadata_filter = {"category": "technology", "date": "2024"}
results = filtered_search("AI ê¸°ìˆ ", documents, metadata_filter)
```

## ğŸ¯ ì‹¤ì œ í™œìš© ì‚¬ë¡€

### 1. ê³ ê° ì„œë¹„ìŠ¤ ì±—ë´‡

```python
# ê³ ê° ì„œë¹„ìŠ¤ RAG ì‹œìŠ¤í…œ
class CustomerServiceRAG:
    def __init__(self):
        self.knowledge_base = self.build_knowledge_base()
        self.qa_chain = self.setup_qa_chain()
    
    def build_knowledge_base(self):
        # FAQ, ë§¤ë‰´ì–¼, ì •ì±… ë¬¸ì„œ ë“± ë¡œë“œ
        documents = load_customer_service_docs()
        return create_vector_store(documents)
    
    def answer_question(self, question):
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.knowledge_base.similarity_search(question, k=3)
        
        # ë‹µë³€ ìƒì„±
        context = "\n".join([doc.page_content for doc in relevant_docs])
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        
        return self.qa_chain.run(prompt)
```

### 2. ë²•ë¥  ë¬¸ì„œ ë¶„ì„

```python
# ë²•ë¥  ë¬¸ì„œ RAG ì‹œìŠ¤í…œ
class LegalDocumentRAG:
    def __init__(self):
        self.legal_kb = self.load_legal_documents()
        self.legal_llm = self.setup_legal_llm()
    
    def load_legal_documents(self):
        # ë²•ë¥ , íŒë¡€, í•´ì„ë¡€ ë“± ë¡œë“œ
        legal_docs = load_legal_corpus()
        return create_legal_vector_store(legal_docs)
    
    def analyze_legal_question(self, question):
        # ê´€ë ¨ ë²•ë¥  ì¡°í•­ ê²€ìƒ‰
        relevant_laws = self.legal_kb.similarity_search(question, k=5)
        
        # ë²•ë¥  ë¶„ì„ ë° ë‹µë³€ ìƒì„±
        return self.generate_legal_analysis(question, relevant_laws)
```

## ğŸš€ ê³ ê¸‰ RAG ê¸°ë²•

### 1. Multi-Query RAG

```python
def multi_query_rag(query):
    # ì—¬ëŸ¬ ê´€ì ì˜ ì§ˆë¬¸ ìƒì„±
    perspectives = [
        f"ê¸°ìˆ ì  ê´€ì ì—ì„œ {query}",
        f"ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ {query}",
        f"ì‚¬ìš©ì ê´€ì ì—ì„œ {query}"
    ]
    
    # ê° ê´€ì ë³„ë¡œ ê²€ìƒ‰
    all_results = []
    for perspective in perspectives:
        results = vector_search(perspective, documents)
        all_results.extend(results)
    
    # ì¤‘ë³µ ì œê±° ë° ì¬ë­í‚¹
    unique_results = remove_duplicates(all_results)
    return rerank_documents(query, unique_results)
```

### 2. Self-RAG

```python
def self_rag(query):
    # ì´ˆê¸° ë‹µë³€ ìƒì„±
    initial_answer = generate_answer(query)
    
    # ë‹µë³€ ê²€ì¦
    verification = verify_answer(initial_answer, query)
    
    if verification.confidence < 0.8:
        # ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
        additional_docs = search_for_gaps(initial_answer, query)
        refined_answer = refine_answer(initial_answer, additional_docs)
        return refined_answer
    
    return initial_answer
```

## ğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

### 1. ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ

```python
def evaluate_retrieval_quality(query, retrieved_docs, relevant_docs):
    # Precision@K
    precision_at_k = len(set(retrieved_docs) & set(relevant_docs)) / len(retrieved_docs)
    
    # Recall@K
    recall_at_k = len(set(retrieved_docs) & set(relevant_docs)) / len(relevant_docs)
    
    # MRR (Mean Reciprocal Rank)
    mrr = 0
    for i, doc in enumerate(retrieved_docs):
        if doc in relevant_docs:
            mrr = 1 / (i + 1)
            break
    
    return {
        "precision": precision_at_k,
        "recall": recall_at_k,
        "mrr": mrr
    }
```

### 2. ìƒì„± í’ˆì§ˆ ì§€í‘œ

```python
def evaluate_generation_quality(generated_answer, reference_answer):
    # BLEU ì ìˆ˜
    bleu_score = calculate_bleu(generated_answer, reference_answer)
    
    # ROUGE ì ìˆ˜
    rouge_score = calculate_rouge(generated_answer, reference_answer)
    
    # ì˜ë¯¸ì  ìœ ì‚¬ë„
    semantic_similarity = calculate_semantic_similarity(
        generated_answer, reference_answer
    )
    
    return {
        "bleu": bleu_score,
        "rouge": rouge_score,
        "semantic_similarity": semantic_similarity
    }
```

## ğŸ¯ ë§ˆë¬´ë¦¬

RAGëŠ” LLMì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ë” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í˜ì‹ ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤.

### í•µì‹¬ í¬ì¸íŠ¸

1. **ê²€ìƒ‰ + ìƒì„±**: ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ë‹µë³€ í’ˆì§ˆ í–¥ìƒ
2. **ì‹¤ì‹œê°„ ì •ë³´**: ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•œ ë‹µë³€ ìƒì„± ê°€ëŠ¥
3. **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ í™œìš© ê°€ëŠ¥
4. **í™˜ê° ë°©ì§€**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€

### ë‹¤ìŒ ë‹¨ê³„

- **ë©€í‹°ëª¨ë‹¬ RAG**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ë¥¼ ëª¨ë‘ í™œìš©
- **ì‹¤ì‹œê°„ RAG**: ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ê³¼ ì—°ë™
- **ê°œì¸í™” RAG**: ì‚¬ìš©ìë³„ ë§ì¶¤í˜• ì§€ì‹ ë² ì´ìŠ¤

RAG ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë”ìš± ì§€ëŠ¥ì ì´ê³  ìœ ìš©í•œ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ì„¸ìš”!

---

**ì°¸ê³  ìë£Œ:**
- [RAG ë…¼ë¬¸](https://arxiv.org/abs/2005.11401)
- [LangChain RAG ê°€ì´ë“œ](https://python.langchain.com/docs/use_cases/question_answering/)
- [Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤](https://www.trychroma.com/)

*ì´ ê¸€ì´ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ ëŒ“ê¸€ë¡œ í”¼ë“œë°±ì„ ë‚¨ê²¨ì£¼ì„¸ìš”! ğŸš€*
